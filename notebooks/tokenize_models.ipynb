{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d145db5d-6285-4e09-a399-9e47c91eb3e8",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h1 align=\"center\"> From Mongo Annotations To BERT Data </h1>\n",
    "\n",
    "<div> In this notebook I present a way to transform the annotations stored in the mongo using the annotation tool made in VUEto feed tensorflow BERT models (or others). For this purpose, only a few librariesare needed. These packages can be changed for others with similar functionalities if it's desired.\n",
    "</div>\n",
    "<div>\n",
    "    \n",
    "- **Spacy:** Used for the basic text tokenization. Other basic text tokenization as White Space Tokenization can be used here.\n",
    "    \n",
    "- **tensorflow_text:** Used to obtain the subtoken words.\n",
    "\n",
    "- **tensorflow:** Fine-tunning BERT models\n",
    "\n",
    "- **tensorflow_hub:** Pre-trainned BERT models\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78f651f-6cff-4bf2-894d-cc35ba47a338",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from extraction.db.db_handler import MongoHandler\n",
    "import sys\n",
    "sys.path.append(\"../../tmp/\")\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c1a237-5662-4d9e-94fd-a0937edac88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mh = MongoHandler()\n",
    "annot = mh.get_all_annotations()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba18e85-e4b1-4444-a23a-7435b7cc2d0f",
   "metadata": {},
   "source": [
    "If the user has filled the mongo DB with annotations you can use the annot variable defined in the last chung. Other else, the annotations will appear as the example of annot shown in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ddb4cd-6a31-4e0f-b012-e5e6019740b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "annot = [\n",
    "    {\n",
    "        'docid': 1,\n",
    "        'text': 'Declaration of final dividend\\n\\nThe Board has declared a final ordinary dividend of 506 cents per share for the year\\n\\nended 30 September 2021. This, together with the interim ordinary dividend of 320\\n\\ncents per share, brings the total dividend for the year to 826 cents. In view of the\\n\\ncompany’s ungeared balance sheet and strong cash generating ability, it has been\\n\\ndecided to determine this year’s total dividend on the company’s adjusted headline\\n\\nearnings. Consequently, HEPS was adjusted to exclude the impact of the product\\n\\nrecall and the civil unrest, which took place in July this year. The Company’s\\n\\ndividend policy of 1.75x cover has therefore been applied to HEPS after the\\n\\naforementioned adjustments.\\n\\n\\nIn accordance with paragraphs 11.17 (a) (i) to (x) and 11.17 (c) of the JSE Listings\\n\\nRequirements, the following additional information is disclosed:\\n\\n\\n   •   The ordinary final dividend has been declared out of income reserves\\n\\n   •   The local Dividends Tax rate is 20% (twenty percent) effective 22 February\\n\\n       2017\\n\\n   •   The gross final dividend amount of 506.00000 cents per ordinary share will be\\n\\n   •   paid to shareholders who are exempt from the Dividends Tax\\n\\n   •   The net final dividend amount of 404.80000 cents per ordinary share will be\\n\\n       paid to\\n\\n   •   shareholders who are liable for the Dividends Tax\\n\\n   •   Tiger Brands has 189 818 926 ordinary shares in issue (which includes 10\\n\\n       326 758 treasury shares)\\n\\n\\n•   Tiger Brands Limited’s income tax reference number is 9325/110/71/7.',\n",
    "        'annotations': [],\n",
    "        'userannotations': [\n",
    "            {\n",
    "                'tagid': 1,\n",
    "                'label': 'per_share_amount',\n",
    "                'label_id': 11,\n",
    "                'start': 83,\n",
    "                'end': 86,\n",
    "                'confidence': 1.0,\n",
    "                'annotatedby': 'user'\n",
    "            },\n",
    "            {\n",
    "                'tagid': 2,\n",
    "                'label': 'currency',\n",
    "                'label_id': 0,\n",
    "                'start': 87,\n",
    "                'end': 92,\n",
    "                'confidence': 1.0,\n",
    "                'annotatedby': 'user'\n",
    "            },\n",
    "            {\n",
    "                'tagid': 3,\n",
    "                'label': 'announcement_date',\n",
    "                'label_id': 7,\n",
    "                'start': 123,\n",
    "                'end': 140,\n",
    "                'confidence': 1.0,\n",
    "                'annotatedby': 'user'\n",
    "            },\n",
    "            {\n",
    "                'tagid': 4,\n",
    "                'label': 'per_share_amount',\n",
    "                'label_id': 11,\n",
    "                'start': 195,\n",
    "                'end': 198,\n",
    "                'confidence': 1.0,\n",
    "                'annotatedby': 'user'\n",
    "            },\n",
    "            {\n",
    "                'tagid': 5,\n",
    "                'label': 'currency',\n",
    "                'label_id': 0,\n",
    "                'start': 200,\n",
    "                'end': 205,\n",
    "                'confidence': 1.0,\n",
    "                'annotatedby': 'user'\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'docid': 1,\n",
    "        'text': 'Declaration of final dividend\\n\\nThe Board has declared a final ordinary dividend of 506 cents per share for the year\\n\\nended 30 September 2021. This, together with the interim ordinary dividend of 320\\n\\ncents per share, brings the total dividend for the year to 826 cents. In view of the\\n\\ncompany’s ungeared balance sheet and strong cash generating ability, it has been\\n\\ndecided to determine this year’s total dividend on the company’s adjusted headline\\n\\nearnings. Consequently, HEPS was adjusted to exclude the impact of the product\\n\\nrecall and the civil unrest, which took place in July this year. The Company’s\\n\\ndividend policy of 1.75x cover has therefore been applied to HEPS after the\\n\\naforementioned adjustments.\\n\\n\\nIn accordance with paragraphs 11.17 (a) (i) to (x) and 11.17 (c) of the JSE Listings\\n\\nRequirements, the following additional information is disclosed:\\n\\n\\n   •   The ordinary final dividend has been declared out of income reserves\\n\\n   •   The local Dividends Tax rate is 20% (twenty percent) effective 22 February\\n\\n       2017\\n\\n   •   The gross final dividend amount of 506.00000 cents per ordinary share will be\\n\\n   •   paid to shareholders who are exempt from the Dividends Tax\\n\\n   •   The net final dividend amount of 404.80000 cents per ordinary share will be\\n\\n       paid to\\n\\n   •   shareholders who are liable for the Dividends Tax\\n\\n   •   Tiger Brands has 189 818 926 ordinary shares in issue (which includes 10\\n\\n       326 758 treasury shares)\\n\\n\\n•   Tiger Brands Limited’s income tax reference number is 9325/110/71/7.',\n",
    "        'annotations': [],\n",
    "        'userannotations': [\n",
    "            {\n",
    "                'tagid': 1,\n",
    "                'label': 'per_share_amount',\n",
    "                'label_id': 11,\n",
    "                'start': 83,\n",
    "                'end': 86,\n",
    "                'confidence': 1.0,\n",
    "                'annotatedby': 'user'\n",
    "            },\n",
    "            {\n",
    "                'tagid': 2,\n",
    "                'label': 'currency',\n",
    "                'label_id': 0,\n",
    "                'start': 87,\n",
    "                'end': 92,\n",
    "                'confidence': 1.0,\n",
    "                'annotatedby': 'user'\n",
    "            },\n",
    "            {\n",
    "                'tagid': 3,\n",
    "                'label': 'announcement_date',\n",
    "                'label_id': 7,\n",
    "                'start': 123,\n",
    "                'end': 140,\n",
    "                'confidence': 1.0,\n",
    "                'annotatedby': 'user'\n",
    "            },\n",
    "            {\n",
    "                'tagid': 4,\n",
    "                'label': 'per_share_amount',\n",
    "                'label_id': 11,\n",
    "                'start': 195,\n",
    "                'end': 198,\n",
    "                'confidence': 1.0,\n",
    "                'annotatedby': 'user'\n",
    "            },\n",
    "            {\n",
    "                'tagid': 5,\n",
    "                'label': 'currency',\n",
    "                'label_id': 0,\n",
    "                'start': 200,\n",
    "                'end': 205,\n",
    "                'confidence': 1.0,\n",
    "                'annotatedby': 'user'\n",
    "            }\n",
    "        ]\n",
    "    }    \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f248ae-2407-4e2a-819a-ceb25887463e",
   "metadata": {},
   "source": [
    "Once the annotations are extracted from the mongo it's necessary to tokenize an align the labels with the tokens for future trainings. To tokenize the text I use spacy but other packages or methods can be used.\n",
    "\n",
    "The following chunk of code uses the offset of the labels to aling the sequence of labels to the sequence of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832ffa76-8a83-4219-ae01-694a873a2ded",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "docs = [nlp(annt[\"text\"]) for annt in annot]\n",
    "doc_tok = []\n",
    "\n",
    "for num, doc in enumerate(docs):\n",
    "    tags = [\n",
    "        (tag[\"start\"],tag[\"end\"], tag[\"label\"]) \n",
    "        for tag in annot[num][\"userannotations\"]\n",
    "    ]\n",
    "    tok = []\n",
    "    \n",
    "    for sent in doc.sents:    \n",
    "        tok_sent = []\n",
    "        last_ent = \"O\"\n",
    "        for token in sent:\n",
    "            token_end = token.idx + len(token.text)\n",
    "            for tag in tags:\n",
    "                if token.idx>=tag[0] and token_end<=tag[1]:\n",
    "                    label = tag[2]\n",
    "                    if last_ent==label:\n",
    "                        label = f\"I-{label}\"\n",
    "                    elif last_ent!=label:\n",
    "                        label = f\"B-{label}\"\n",
    "                    break\n",
    "                else:\n",
    "                    label = \"O\"\n",
    "            last_ent = label[2:]\n",
    "            t = {\n",
    "                    \"token\": token,\n",
    "                    \"start\": token.idx,\n",
    "                    \"end\": token_end,\n",
    "                    \"label\": label,\n",
    "                }\n",
    "            tok_sent.append(t)\n",
    "        tok.append(tok_sent)\n",
    "    doc_tok.append(tok)\n",
    "\n",
    "# if using the annot example:\n",
    "assert len(doc_tok)==2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77cbe2d-bfd5-45c7-b1e3-6d04a4247f1a",
   "metadata": {},
   "source": [
    "For the example I'm going to use the model pre-trained offered by Tensorflow Hub *https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/2*. This selection has not been based in some kind of results or any other performance metric. It's just used to show the concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cb4d13-b6e9-4add-bb21-aaad92359419",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bert_layer = hub.KerasLayer(\n",
    "    'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/2',\n",
    "    trainable=False\n",
    ")\n",
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer =  text.BertTokenizer(f\"../..{vocab_file.decode()}\")\n",
    "#tokenizer = text.BertTokenizer(\"/workspace/data/tf_vocab/vocab.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f1df9e-8ae1-4e6b-8cc6-54877d2a1281",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize_with_labels(tokens, text_labels):\n",
    "    tokenized_sentence_id = []\n",
    "    labels = []\n",
    "\n",
    "    for word, label in zip(tokens, text_labels):\n",
    "        if isinstance(word, str):\n",
    "            tokenized_word = tokenizer.tokenize(word)\n",
    "        else:\n",
    "            if word.text == \"\\n\\n\":\n",
    "                continue\n",
    "            tokenized_word = tokenizer.tokenize(word.text)\n",
    "        try:\n",
    "            tokenized_word = tokenized_word.to_list()[0][0]\n",
    "        except IndexError:\n",
    "            continue\n",
    "        n_subwords = len(tokenized_word)\n",
    "        \n",
    "        tokenized_sentence_id.extend(tokenized_word)\n",
    "        labels.extend([label] * n_subwords)\n",
    "\n",
    "    return tokenized_sentence_id, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545dbb32-139c-4eef-a2bc-b167bf0f6234",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = [[token[\"token\"] for token in sent] for doc in doc_tok for sent in doc]\n",
    "labels = [[token[\"label\"] for token in sent] for doc in doc_tok for sent in doc]\n",
    "\n",
    "tokenized_texts_and_labels = [\n",
    "    tokenize_with_labels(sent, labs)\n",
    "    for sent, labs in zip(sents, labels)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e585d5-0df9-4ddf-b396-ec8c047dfaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_texts = [token_label_pair[0] for token_label_pair in tokenized_texts_and_labels]\n",
    "labels = [token_label_pair[1] for token_label_pair in tokenized_texts_and_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6e461c-6205-4d00-b9f8-0beb76a4bf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = [\n",
    "    'B-currency','B-symbol','B-company_name',\n",
    "    'B-security_description','B-type','B-frequency',\n",
    "    'B-announcement_date','B-record_date','B-ex_date',\n",
    "    'B-payment_date','B-per_share_amount',\n",
    "    'I-currency','I-symbol','I-company_name',\n",
    "    'I-security_description','I-type','I-frequency',\n",
    "    'I-announcement_date','I-record_date','I-ex_date',\n",
    "    'I-payment_date','I-per_share_amount','O',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da02db2-53e0-4794-bb8b-41bff03be3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 68\n",
    "tags.append(\"PAD\")\n",
    "tag2idx = {t: i for i, t in enumerate(tags)}\n",
    "\n",
    "tags = pad_sequences(\n",
    "    [[tag2idx.get(l) for l in lab] for lab in labels],\n",
    "    maxlen=maxlen, value=tag2idx[\"PAD\"], padding=\"post\",\n",
    "    dtype=\"long\", truncating=\"post\"\n",
    ")\n",
    "\n",
    "n_tags = len(tag2idx)\n",
    "pad_tags = [to_categorical(i, num_classes=n_tags) for i in tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c470a99f-1e20-477b-8afe-f0e572d740b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tf.convert_to_tensor(\n",
    "    pad_sequences(\n",
    "        [txt for txt in tokenized_texts],\n",
    "        maxlen=maxlen, dtype=\"long\", value=0.0,\n",
    "        truncating=\"post\", padding=\"post\"\n",
    "    )\n",
    ")\n",
    "attention_masks = tf.convert_to_tensor([[float(i != 0.0) for i in ii] for ii in input_ids.numpy()])\n",
    "input_type_ids = tf.convert_to_tensor([[0 for _ in ii] for ii in input_ids.numpy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b17316e-0a05-4f4e-a449-5ae2d40cc107",
   "metadata": {},
   "source": [
    "Once the documents are segmented by sentence, tokenized and the padding is added is necessary to create the input structure for BERT models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea398b3-3cad-46f5-813b-f07bdaf71e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {\n",
    "    \"input_word_ids\":input_ids,\n",
    "    \"input_mask\":attention_masks,\n",
    "    \"input_type_ids\":input_type_ids,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693235d5-74f3-48cb-adde-79a24040b72c",
   "metadata": {},
   "source": [
    "For the model, all information included in the PAD token is irrelevant. To avoid the computation of the loss in those PAD tokens the following function will transform the error in those tokens to 0. By doing this mutation on the loss function the model should disregard the error in those elements and turn its attention to the actual text of the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0ebfc0-13b9-4339-afc0-3d5278ceed61",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.CategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    print(real)\n",
    "    print(pred)\n",
    "    reales = tf.math.reduce_sum(\n",
    "       tf.cast(tf.equal(real, to_categorical(n_tags-1, num_classes=n_tags)), tf.float32),\n",
    "       axis=-1, keepdims=False, name=None\n",
    "    )\n",
    "    mask = tf.equal(reales,n_tags)\n",
    "    mask = tf.math.logical_not(mask)\n",
    "    loss_ = loss_object(real,pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_*= mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e5fff0-9c26-4cc1-846b-309a230e4e66",
   "metadata": {},
   "source": [
    "Here, the model is just for demostration purposes. To show how the inputs are introduced an the output generated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeab57fe-044a-49fd-ae7e-a84776121511",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    \n",
    "    input_word_ids = tf.keras.layers.Input(\n",
    "        shape=(maxlen,),\n",
    "        dtype=tf.int32,\n",
    "        name=\"input_word_ids\"\n",
    "    )\n",
    "    input_mask = tf.keras.layers.Input(\n",
    "        shape=(maxlen,),\n",
    "        dtype=tf.int32,\n",
    "        name=\"input_mask\"\n",
    "    )\n",
    "    input_type_ids = tf.keras.layers.Input(\n",
    "        shape=(maxlen,),\n",
    "        dtype=tf.int32,\n",
    "        name=\"input_type_ids\"\n",
    "    )\n",
    "    pooled_output, sequence_output = bert_layer(\n",
    "        [input_word_ids, input_mask, input_type_ids]\n",
    "    )\n",
    "    \n",
    "    output = tf.keras.layers.TimeDistributed(\n",
    "        tf.keras.layers.Dense(64, activation=\"tanh\")\n",
    "    )(sequence_output)\n",
    "   \n",
    "    output = tf.keras.layers.TimeDistributed(\n",
    "        tf.keras.layers.Dense(len(tag2idx.keys()), activation=\"softmax\")\n",
    "    )(output)\n",
    "\n",
    "    model = tf.keras.Model(\n",
    "      inputs={\n",
    "        'input_word_ids': input_word_ids,\n",
    "        'input_mask': input_mask,\n",
    "        'input_type_ids': input_type_ids}, \n",
    "      outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bb51cf-cdbb-4012-a6bf-ca325698d63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead5a450-7099-4d36-8c57-52eda305d93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=loss_function,\n",
    "    metrics=[tf.keras.metrics.Precision()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2e589c-1612-4663-804c-78505528ae1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "history = classifier_model.fit(\n",
    "    dataset,\n",
    "    np.array(pad_tags),\n",
    "    epochs=200,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da8ef2e-0aef-4f39-88ff-1ca64f49d043",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bert_raw_result = classifier_model(dataset)\n",
    "print(bert_raw_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2660203-d2e4-470a-af0c-5a0f7a4cfa40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bert_result=[np.argmax(i) for i in bert_raw_result[0]]\n",
    "bert_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd5897d-ff7f-46c9-b2ed-2e4ba8d8d82a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#tokenizer.detokenize(tokenizer.tokenize('Declaration'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
